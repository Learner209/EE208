# <center>小型搜索引擎</center> #

<center>姓名：刘洺皓 学号：521030910014 班级：F2103001</center>

## 1.项目概述 ## 
&emsp;本项目旨在实现一个小型搜索引擎，其实现的基本原理如下：
* 利用python的urllib库和HTTP Base Handler库来实现网页爬取
* 利用lxml库构建网页树，解析网页内容，提取相关有用信息，使用jieba分词，亮点在于构建了一个具有打分机制的，兼容中英文，基本能够适配大量网站的对于图片的描述信息的提取的函数。
* 利用开源的Lucene引擎进行利用WhitespaceAnanlyzer实现中文分词建立索引和读取索引，以及一些增删改查和不同字符串搜索方式的实现。
*  构建了一个*Web*前端，实现了简单的主页，搜索结果呈现页和图片排版页，并拓展了一些如搜索历史查看，快捷方式增删，日期范围划定，加载页面呈现等功能。
* 接触并自己简略实现了*Simhash*文本查重,*Pagerank*,*Textrank*等算法。
*  再尝试寻找*Pagerank*的优化时，在*Google Scholar*上找到了一篇 *Adaptive methods for the computation of PageRank* 并咀嚼消化，最后也自己进行了实现。


## 2.项目环境 ##
本项目具体使用的工具如下:
* Docker中的ee208镜像(含Pylucene库)
* docker初始环境未安装的库:aiohttp(与asyncio实现异步协程爬虫),lxml,bitarray, func_set_timeout, jieba
* Python(Vscode)

## 3.项目片段 ##

### 3.1.1 中文的分词  ###
&emsp;利用jieba分词，可以利用 *cut_for_serach* 或者 *lcut_for_serach* 方法尽可能地切词，以提高召回率，同时在分析网页时也需要将原来的中文内容按照分词结果切开，中间留有空格，以期 _StandardAnalyzer_ 能够像英文切词一样将中文以空格切开，但是实际证明 _StandardAnalyzer_ 对待中文还是会一个一个字切开，所以这里改用 _WhitespaceAnalyzer_ 进行切词，提高对短语的完整性保留。

### 3.1.2 问题分析 ###

&emsp; 


### 3.1.2 代码与运行结果 ###
代码展示如下: 

```

```

运行结果如图:



#### 分析与反思： #### 


***


### 3.2.1 索引的建立 ### 
1. Indexer问题
    在使用 docker container 时经常会发现会 killed 现象，初步判断是内存溢出进程自动被杀死，第一部我先检查了线程问题，我使用了 *threading.active_count* 发现我的活跃线程数在200以下，且线程名字都能对应上我初始化线程的线程名字，应该不是线程问题，接下来考虑内存使用情况，我对代码进行了内存的部分优化，并且尝试降低线程数以降低内存负载，但是还是会出现 KILLED 现象，之后我又尝试了 docker Desktop 中提升容器的 CPU  CORE， DISK IMAGE SIZE和 Memory 与 Swap 容量， 并且经过命令行 top 指令查看动态内存使用情况，发现 python3 占用的内存并不足以达到使该进程被 killed的程度，反而是几个node进程占用了大量内存，在未运行python3 时尝试 kill PID 但是并没有实效，接下来尝试本机安装 *pylucene* 但是意料之中地受挫而归，最终的解决方法是利用 *OPEN_OR_APPEND* 方法来打开 *SimpleFSDirectory* 进行index多次来达到网页数量要求。
2. *IndexFiles*
* 自定义field可以利用fixed好的 *TextField*, *StringField*, *NumericField*但是这样的话当调用`indexReader.getTermVectors`的话会无法获得相应的权词向量，只有当是自定义field类型且`setStoreTermVectors(True)`时才会正常获得*Termvectors*
* 同时发现当设置STORED为TRUE时，内存空间会被大量消耗，导致top命令观察到的剩余空闲内存急剧减少以致进程被快速kill，所以在container中进行实验时要*SetStored(False)*,但这样就需要在展示结果时重新获取，无论是读取本地已存储文件还是重新发起网页请求，复刻parse结果都会延长程序运行时间，这是又一个时间与空间的权衡，在本次试验中因为考虑到container的运行效果不及裸机，所以index时不存储内容，需要在查询时重新获取内容，会造成一定的时间浪费。
3. *SearchFiles*
* 为了节省建立索引空间，在每次返回一个查询成功的结果时，都会重新进行网页请求。所以构建了一个工具函数presentQuery函数，作用是每次接收一个目标网址发起请求，进行搜索结果的查重，建立文档并存储网页内容。然后返回本地文件存储地址。
presentQuery函数需要注意以下几点:
1. 为了实现在文档当中快速定位查询成功的结果所在的offset,这里运用了lucene的API,使用了*SimpleHTMLFormatter*,*QueryScorer*类来实例化*Highlighter*,并且为了文本切割定位使用了setTextFragmenter,最终调用getBestFragment来获得相对应的文本片段
2. 为了实现搜索结果的查重(因为即使网址大相径庭，搜索仍能获得相似结果），在整个循环过程当中维护一个已查重的。*contentPool*,并且利用字符串的*Differ*库解析两个字符串的比较返回结果，这里调用了TextProcessor中所写的*common*工具函数，利用*differlib*所产生的字符串的-+号数量之差来判断两个字符串的文本差异
3. 在获得目标网页响应内容之后，按照网页网址的域名*netloc,path,params,query*来逐级创建目录存储内容，并在且返回创建路径
4. 为了保证能够准确获取网页，屏蔽网址环境变更所带来的不确定性。在调用*crawler.py*的`standard_request`函数获取网页内容不符合要求时将会再使用asyncio异步爬虫进行爬取，因为协程异步爬虫充分调用cpu资源，所以带来的时间耗费可以忽略不计
### 3.2.1 问题分析 ### 


&emsp; 
 
### 3.2.3 代码与运行结果 ###  
代码展示如下: 


```
```

运行结果如下：



#### 分析与思考: ####


 ***

### 3.3.1 网页解析 ### 
1. 清除 HTML 标签
   &emsp;显示 _nlkt_ 库已经 deprecate *clean_html* 方法，这里从 nlkt package 中沿用了原来的方法、
2. 为了提高匹配精确度，这里对 *jieba* 使用了停用词，以过滤常见词汇，停用词文件名称为 cnstopwords.txt, 在 instantiating *Textprocessing* 类时将会自动读取停用词并且将停用词作为类的静态变量给不同的实例化对象共享以节省空间。
3. 网页分析中考虑语言问题，这里使用 lxml 库来解析， 并运用 xpath 或 cssselect(需要 pip另外安装) 选择 <meta> 标签中的 lang属性， 若lang为en-US,则在分词时会不使用jieba而是将英文单词作为 raw material 传入 _WhitespaceAnalyzer_ , 若lang为 zh-CN ,则在分词时会使用jieba预留空格， 调用 *TextProcessor* 中的 `TokenizeChinese` 方法， 若lang中没有表明文件语言类型，那么将调用 *TextProcessor* 中的 `TokenizeMixedLan` 方法。
   * 查询 <meta> 中 lang，可用`"//head/meta[contains(@content,'charset=')]"`作为xpath.
   * *TextProcessor* 中的 `TokenizeChinese`方法：可利用正则表达式 '[\u4e00-\u9fa5，。、？！《》‘’“”：；]+' 匹配，然后对中文的meaningful segment进行切分，为作扩展，`TokenizeChinese` 方法提供了是否使用jieba分词及是否利用 *jieba* 的搜索引擎匹配模式的接口
   * *TextProcessor* 中的 `TokenizeMixedLan` 方法目的在于保留除中文字符与标点符号外的内容，但将中文替换为分词后的结果，为了节省空间，`TokenizeMixedLan` 方法比 `TokenizeChinese` 方法 多做的工作在循环中记录了上一次的分词终止的位置，然后将 last_tokenized 到当前分词开始处的内容保留，重复更新结果变量。
4.  *TextProcessing* 中的 `parseContents` 方法
    为加强文本与网页的相关性，这里使用了 *lxml.html.clean.Cleaner* 类，利用 `Cleaner(style=True, scripts=True, links = False, remove_tags = remove_tags)` 方法来定位到我们想要的内容，我清除掉了 footer, css样式，脚本文件（虽然无法排除有少数网页的HTML内容就放在script中进行动态渲染，或者以 Ajax 请求交换 JSON 数据来得到脚本渲染网页时的原材料）,同时我也只保留了body部分的网页内容。
5. parse 网页中的外链
   xpath中的写法为 `'//a[@href and @rel != "nofollow"] | //a[@href] | //iframe[@src]'` 这里不处理网页的 nofollow 标签，因为做的假设是这些 nofollow links对网页的贡献度不大，这里也就爬取时滤过，网页中的链接分好多种，这里只针对转向其他有效网页的外链，所以首先去除 `href:"javascript:void(...)`形式，其次判断是否是锚点，即是否startswith('#'),最后判断该链接是否是资源下载链接，作为另一种连接形式保存，这里使用正则表达式匹配后缀名实现。
6. 如果网页中有 `<base>` 标签，那么可以将其作为基准网页定位相对地址。
7. 编码问题：有些网站的 charset 与实际网页内容使用的编码方式不符，常见在某些 gbk 与 gb2312 为主的网站,所以要对 *UnicodeError* 进行异常捕获。
8. `TokenizeMixedLan` 函数中为了提高效率，`jieba.enable_parallel(4)` 开启了多线程的并行分词模式，其底层工作原理是将待分词内容分割为几小块，对每一块进行分词，这种方法对待长篇幅文本效率会显著提升，但是对待短篇幅文本例如一个网站的内容来说，其并行分词的效率提升还不足以弥补启动多线程并关闭线程所耗费的时间，这里可以选择把`jieba.disable_parallel()`放在所有分词行为的末端，而在 *TextProcessing* 类初始化时就调用`jieba.enable_parallel(4)`这样可以减少开关线程造成的时间浪费。在实际运行jieba分词过程中又出现了一个新问题，就是当jieba模块被调用好之后，相当于给jieba的分词器上了锁，在多线程共同请求分词时，只有一个线程会获得锁，所以在任意时刻真正只有一个线程是在进行切词工作，这无疑是降低了程序的效率，退化成了单线程工作，所以如何考虑将*jieba*的分词行为在并行线程中展开是必要的问题。同时,多线程运用*jieba*分词而非使用*jieba*所提供的*native API*`enable_parallel` `disable_parallel()` 也会使得线程工作变得混乱，在实际运行过程中就经常遇*ZeroDivisionError*, *ValueError*,并且报错信息显示在 *jieba/__init__.py* 中的 `lcut(pool, map)` 的一行，并且trigger了*multiprocessing*库中的错误，说明*jieba*并非thread-safe,或者对于多线程调用的兼容性不好，这里的处理方法目前只能是异常捕获，可以在以后尝试阅读源码并修改。


### 3.3.2 问题分析 ### 


&emsp;


### 3.3.3 代码与运行结果 ### 


 代码展示如下: 

```

```

运行结果如下：



#### 分析与反思： ####

&emsp;

## 4.拓展思考 ###

&emsp;

